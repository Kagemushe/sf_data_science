{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ac62707",
   "metadata": {},
   "outputs": [],
   "source": [
    "#импортируем библиотеки\n",
    "import numpy as np #для матричных вычислений\n",
    "import pandas as pd #для анализа и предобработки данных\n",
    "import matplotlib.pyplot as plt #для визуализации\n",
    "import seaborn as sns #для визуализации\n",
    "\n",
    "from sklearn import linear_model #линейные моделиё\n",
    "from sklearn import tree #деревья решений\n",
    "from sklearn import ensemble #ансамбли\n",
    "from sklearn import metrics #метрики\n",
    "from sklearn import preprocessing #предобработка\n",
    "from sklearn.model_selection import train_test_split #сплитование выборки\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import hyperopt\n",
    "from hyperopt import hp, fmin, tpe, Trials\n",
    "\n",
    "import optuna\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c5435f2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Activity</th>\n",
       "      <th>D1</th>\n",
       "      <th>D2</th>\n",
       "      <th>D3</th>\n",
       "      <th>D4</th>\n",
       "      <th>D5</th>\n",
       "      <th>D6</th>\n",
       "      <th>D7</th>\n",
       "      <th>D8</th>\n",
       "      <th>D9</th>\n",
       "      <th>...</th>\n",
       "      <th>D1767</th>\n",
       "      <th>D1768</th>\n",
       "      <th>D1769</th>\n",
       "      <th>D1770</th>\n",
       "      <th>D1771</th>\n",
       "      <th>D1772</th>\n",
       "      <th>D1773</th>\n",
       "      <th>D1774</th>\n",
       "      <th>D1775</th>\n",
       "      <th>D1776</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.497009</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.132956</td>\n",
       "      <td>0.678031</td>\n",
       "      <td>0.273166</td>\n",
       "      <td>0.585445</td>\n",
       "      <td>0.743663</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.366667</td>\n",
       "      <td>0.606291</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.111209</td>\n",
       "      <td>0.803455</td>\n",
       "      <td>0.106105</td>\n",
       "      <td>0.411754</td>\n",
       "      <td>0.836582</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0.033300</td>\n",
       "      <td>0.480124</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.209791</td>\n",
       "      <td>0.610350</td>\n",
       "      <td>0.356453</td>\n",
       "      <td>0.517720</td>\n",
       "      <td>0.679051</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.538825</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.196344</td>\n",
       "      <td>0.724230</td>\n",
       "      <td>0.235606</td>\n",
       "      <td>0.288764</td>\n",
       "      <td>0.805110</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.517794</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.494734</td>\n",
       "      <td>0.781422</td>\n",
       "      <td>0.154361</td>\n",
       "      <td>0.303809</td>\n",
       "      <td>0.812646</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1777 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Activity        D1        D2    D3   D4        D5        D6        D7  \\\n",
       "0         1  0.000000  0.497009  0.10  0.0  0.132956  0.678031  0.273166   \n",
       "1         1  0.366667  0.606291  0.05  0.0  0.111209  0.803455  0.106105   \n",
       "2         1  0.033300  0.480124  0.00  0.0  0.209791  0.610350  0.356453   \n",
       "3         1  0.000000  0.538825  0.00  0.5  0.196344  0.724230  0.235606   \n",
       "4         0  0.100000  0.517794  0.00  0.0  0.494734  0.781422  0.154361   \n",
       "\n",
       "         D8        D9  ...  D1767  D1768  D1769  D1770  D1771  D1772  D1773  \\\n",
       "0  0.585445  0.743663  ...      0      0      0      0      0      0      0   \n",
       "1  0.411754  0.836582  ...      1      1      1      1      0      1      0   \n",
       "2  0.517720  0.679051  ...      0      0      0      0      0      0      0   \n",
       "3  0.288764  0.805110  ...      0      0      0      0      0      0      0   \n",
       "4  0.303809  0.812646  ...      0      0      0      0      0      0      0   \n",
       "\n",
       "   D1774  D1775  D1776  \n",
       "0      0      0      0  \n",
       "1      0      1      0  \n",
       "2      0      0      0  \n",
       "3      0      0      0  \n",
       "4      0      0      0  \n",
       "\n",
       "[5 rows x 1777 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('C:\\IDE\\data\\Block_4\\_train_sem09 (1).csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b94dd5cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.drop(['Activity'], axis=1)\n",
    "y = data['Activity']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "224eef86",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state = 42, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cce09911",
   "metadata": {},
   "source": [
    "## <center>1.Модели логистической регрессии и случайного леса методом GridSearchCV<center>\n",
    "### <center>Логистическая регрессия<center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "76d0f0bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Smoking Shop\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:540: FitFailedWarning: \n",
      "70 fits failed out of a total of 280.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "56 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Smoking Shop\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Smoking Shop\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\base.py\", line 1466, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"c:\\Users\\Smoking Shop\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\base.py\", line 666, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"c:\\Users\\Smoking Shop\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 95, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'penalty' parameter of LogisticRegression must be a str among {'l2', 'elasticnet', 'l1'} or None. Got 'none' instead.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "12 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Smoking Shop\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Smoking Shop\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\base.py\", line 1466, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"c:\\Users\\Smoking Shop\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\base.py\", line 666, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"c:\\Users\\Smoking Shop\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 95, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'penalty' parameter of LogisticRegression must be a str among {'l2', 'l1', 'elasticnet'} or None. Got 'none' instead.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "2 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Smoking Shop\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Smoking Shop\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\base.py\", line 1466, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"c:\\Users\\Smoking Shop\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\base.py\", line 666, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"c:\\Users\\Smoking Shop\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 95, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'penalty' parameter of LogisticRegression must be a str among {'elasticnet', 'l1', 'l2'} or None. Got 'none' instead.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "c:\\Users\\Smoking Shop\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_search.py:1103: UserWarning: One or more of the test scores are non-finite: [0.763      0.762             nan        nan 0.76233333 0.76133333\n",
      "        nan        nan 0.755      0.75433333        nan        nan\n",
      " 0.757      0.757             nan        nan 0.753      0.75333333\n",
      "        nan        nan 0.75133333 0.75133333        nan        nan\n",
      " 0.75066667 0.75133333        nan        nan 0.743      0.743\n",
      " 0.76033333 0.76266667 0.76       0.75933333 0.76066667 0.76166667\n",
      " 0.76466667 0.76466667 0.754      0.75433333 0.764      0.76366667\n",
      " 0.75733333 0.75633333 0.76233333 0.76233333 0.75333333 0.753\n",
      " 0.75833333 0.756      0.75033333 0.751      0.75766667 0.75433333\n",
      " 0.75       0.751     ]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_score на тестовом наборе: 0.78\n",
      "Наилучшие значения гиперпараметров: {'C': 0.3, 'penalty': 'l1', 'solver': 'liblinear'}\n"
     ]
    }
   ],
   "source": [
    "# Задаем сетку гиперпараметров\n",
    "param_grid = [\n",
    "              {'penalty': ['l2', 'none'] , # тип регуляризации\n",
    "              'solver': ['lbfgs', 'sag'], # алгоритм оптимизации\n",
    "               'C': [0.01, 0.1, 0.3, 0.5, 0.7, 0.9, 1]}, # уровень силы регуляризации\n",
    "              \n",
    "              {'penalty': ['l1', 'l2'] ,\n",
    "              'solver': ['liblinear', 'saga'],\n",
    "               'C': [0.01, 0.1, 0.3, 0.5, 0.7, 0.9, 1]}\n",
    "]\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=linear_model.LogisticRegression(random_state=42, max_iter=1000), \n",
    "    param_grid=param_grid, \n",
    "    cv=5, \n",
    "    n_jobs = -1\n",
    ")  \n",
    " \n",
    "\n",
    "grid_search.fit(X_train, y_train) \n",
    "y_test_pred = grid_search.predict(X_test)\n",
    "print('f1_score на тестовом наборе: {:.2f}'.format(metrics.f1_score(y_test, y_test_pred)))\n",
    "print(\"Наилучшие значения гиперпараметров: {}\".format(grid_search.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7df3a599",
   "metadata": {},
   "source": [
    "### <center>Рандомный лес<center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "59238382",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_score на тестовом наборе: 0.81\n",
      "Наилучшие значения гиперпараметров: {'max_depth': 15, 'min_samples_leaf': 2, 'n_estimators': 130}\n"
     ]
    }
   ],
   "source": [
    "# Задаем сетку гиперпараметров\n",
    "param_grid = {'n_estimators': list(range(100, 300, 10)),\n",
    "              'min_samples_leaf': list(range(2, 7, 1)),\n",
    "              'max_depth': list(range(5, 20, 2))}\n",
    "              \n",
    "             \n",
    "              \n",
    "            \n",
    "grid_search_forest = GridSearchCV(\n",
    "    estimator=ensemble.RandomForestClassifier(random_state=42), \n",
    "    param_grid=param_grid, \n",
    "    cv=5, \n",
    "    n_jobs = -1\n",
    ")  \n",
    "\n",
    "grid_search_forest.fit(X_train, y_train) \n",
    "y_train_pred = grid_search_forest.predict(X_train)\n",
    "y_test_pred = grid_search_forest.predict(X_test)\n",
    "print('f1_score на тестовом наборе: {:.2f}'.format(metrics.f1_score(y_test, y_test_pred)))\n",
    "print(\"Наилучшие значения гиперпараметров: {}\".format(grid_search_forest.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0537bce6",
   "metadata": {},
   "source": [
    "## <center>2.Модели логистической регрессии и рандомного леса методом RandomizedSearchCV<center>\n",
    "### <center>Логистическая регрессия<center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8ad1b972",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Smoking Shop\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:540: FitFailedWarning: \n",
      "20 fits failed out of a total of 50.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "1 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Smoking Shop\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Smoking Shop\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\base.py\", line 1466, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"c:\\Users\\Smoking Shop\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\base.py\", line 666, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"c:\\Users\\Smoking Shop\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 95, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'penalty' parameter of LogisticRegression must be a str among {'l1', 'l2', 'elasticnet'} or None. Got 'none' instead.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "2 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Smoking Shop\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Smoking Shop\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\base.py\", line 1466, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"c:\\Users\\Smoking Shop\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\base.py\", line 666, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"c:\\Users\\Smoking Shop\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 95, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'penalty' parameter of LogisticRegression must be a str among {'l2', 'l1', 'elasticnet'} or None. Got 'none' instead.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "6 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Smoking Shop\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Smoking Shop\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\base.py\", line 1466, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"c:\\Users\\Smoking Shop\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\base.py\", line 666, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"c:\\Users\\Smoking Shop\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 95, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'penalty' parameter of LogisticRegression must be a str among {'l2', 'elasticnet', 'l1'} or None. Got 'none' instead.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "11 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Smoking Shop\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Smoking Shop\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\base.py\", line 1466, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"c:\\Users\\Smoking Shop\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\base.py\", line 666, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"c:\\Users\\Smoking Shop\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 95, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'penalty' parameter of LogisticRegression must be a str among {'elasticnet', 'l1', 'l2'} or None. Got 'none' instead.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "c:\\Users\\Smoking Shop\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_search.py:1103: UserWarning: One or more of the test scores are non-finite: [       nan 0.743      0.75033333 0.75133333 0.75433333 0.76033333\n",
      "        nan        nan 0.756             nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_score на тестовом наборе: 0.78\n",
      "Наилучшие значения гиперпараметров: {'solver': 'liblinear', 'penalty': 'l2', 'C': 0.01}\n"
     ]
    }
   ],
   "source": [
    "param_distributions = [\n",
    "              {'penalty': ['l2', 'none'] , # тип регуляризации\n",
    "              'solver': ['lbfgs', 'sag'], # алгоритм оптимизации\n",
    "               'C': [0.01, 0.1, 0.3, 0.5, 0.7, 0.9, 1]}, # уровень силы регуляризации\n",
    "              \n",
    "              {'penalty': ['l1', 'l2'] ,\n",
    "              'solver': ['liblinear', 'saga'],\n",
    "               'C': [0.01, 0.1, 0.3, 0.5, 0.7, 0.9, 1]}\n",
    "              ]\n",
    "            \n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=linear_model.LogisticRegression(random_state=42, max_iter=1000), \n",
    "    param_distributions=param_distributions, \n",
    "    cv=5, \n",
    "    n_iter = 10, \n",
    "    n_jobs = -1\n",
    ")  \n",
    " \n",
    " \n",
    "random_search.fit(X_train, y_train) \n",
    "y_test_pred = random_search.predict(X_test)\n",
    "print('f1_score на тестовом наборе: {:.2f}'.format(metrics.f1_score(y_test, y_test_pred)))\n",
    "print(\"Наилучшие значения гиперпараметров: {}\".format(random_search.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3b882db",
   "metadata": {},
   "source": [
    "### <center>Рандомный лес<center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "57b5d4fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_score на тестовом наборе: 0.80\n",
      "Наилучшие значения гиперпараметров: {'n_estimators': 170, 'min_samples_leaf': 2, 'max_depth': 17}\n"
     ]
    }
   ],
   "source": [
    "# Задаем сетку гиперпараметров\n",
    "param_distributions = {'n_estimators': list(range(100, 300, 10)),\n",
    "              'min_samples_leaf': list(range(2, 7, 1)),\n",
    "              'max_depth': list(range(5, 20, 2))}\n",
    "              \n",
    "             \n",
    "              \n",
    "            \n",
    "random_search_forest = RandomizedSearchCV(\n",
    "    estimator=ensemble.RandomForestClassifier(random_state=42), \n",
    "    param_distributions=param_distributions, \n",
    "    cv=5, \n",
    "    n_jobs = -1\n",
    ")  \n",
    "\n",
    "random_search_forest.fit(X_train, y_train) \n",
    "y_train_pred = random_search_forest.predict(X_train)\n",
    "y_test_pred = random_search_forest.predict(X_test)\n",
    "print('f1_score на тестовом наборе: {:.2f}'.format(metrics.f1_score(y_test, y_test_pred)))\n",
    "print(\"Наилучшие значения гиперпараметров: {}\".format(random_search_forest.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9accf1f",
   "metadata": {},
   "source": [
    "## <center>3.Модели логистической регрессии и рандомного леса методом Hyperopt<center>\n",
    "### <center>Логистическая регрессия<center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "2333a70e",
   "metadata": {},
   "outputs": [],
   "source": [
    "space = {'C': hp.quniform('C', 0.01, 1, 0.05)}\n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "c5f19908",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state = 42\n",
    "def hyperopt(params, cv=5, X=X_train, y=y_train, random_state=random_state):\n",
    "    \n",
    "    params = { 'C': params['C']}\n",
    "    # используем эту комбинацию для построения модели\n",
    "    model = linear_model.LogisticRegression(**params, random_state=random_state)\n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "    score = cross_val_score(model, X, y, cv=cv, scoring=\"f1\", n_jobs=-1).mean()\n",
    "    \n",
    "    \n",
    "    # метрику необходимо минимизировать, поэтому ставим знак минус\n",
    "    return -score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "e123be61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:43<00:00,  2.17s/trial, best loss: -0.7904446144717193]\n",
      "Наилучшие значения гиперпараметров {'C': 0.05}\n"
     ]
    }
   ],
   "source": [
    "# начинаем подбор гиперпараметров\n",
    "\n",
    "trials = Trials() # используется для логирования результатов\n",
    "\n",
    "best=fmin(hyperopt, # наша функция \n",
    "          space=space, # пространство гиперпараметров\n",
    "          algo=tpe.suggest, # алгоритм оптимизации, установлен по умолчанию, задавать необязательно\n",
    "          max_evals=20, # максимальное количество итераций\n",
    "          trials=trials, # логирование результатов\n",
    "          rstate=np.random.default_rng(random_state) # фиксируем для повторяемости результата\n",
    "         )\n",
    "print(\"Наилучшие значения гиперпараметров {}\".format(best))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "71670e8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_score на тестовом наборе: 0.79\n"
     ]
    }
   ],
   "source": [
    "model = linear_model.LogisticRegression(\n",
    "    random_state=random_state, \n",
    "    C=float(best['C'])\n",
    ")\n",
    "model.fit(X_train, y_train)\n",
    "y_train_pred = model.predict(X_train)\n",
    "y_test_pred = model.predict(X_test)\n",
    "print('f1_score на тестовом наборе: {:.2f}'.format(metrics.f1_score(y_test, y_test_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6d62560",
   "metadata": {},
   "source": [
    "### <center>Рандомный лес<center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "eefd3dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Задаем сетку гиперпараметров\n",
    "space = {'n_estimators': hp.quniform('n_estimators', 100, 300, 10),\n",
    "              'min_samples_leaf': hp.quniform('min_samples_leaf', 2, 7, 1),\n",
    "              'max_depth': hp.quniform('max_depth', 5, 20, 2)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "806ce3d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# зафксируем random_state\n",
    "random_state = 42\n",
    "def hyperopt_rf(params, cv=5, X=X_train, y=y_train, random_state=random_state):\n",
    "    \n",
    "    params = {'n_estimators': int(params['n_estimators']), \n",
    "              'max_depth': int(params['max_depth']), \n",
    "             'min_samples_leaf': int(params['min_samples_leaf'])\n",
    "              }\n",
    "  \n",
    "    # используем эту комбинацию для построения модели\n",
    "    model = ensemble.RandomForestClassifier(**params, random_state=random_state)\n",
    "\n",
    "    # обучаем модель\n",
    "    score = cross_val_score(model, X, y, cv=cv, scoring=\"f1\", n_jobs=-1).mean()\n",
    "\n",
    "    # метрику необходимо минимизировать, поэтому ставим знак минус\n",
    "    return -score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "9df9d74e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [01:39<00:00,  4.97s/trial, best loss: -0.8174708361609223]\n",
      "Наилучшие значения гиперпараметров {'max_depth': 14.0, 'min_samples_leaf': 2.0, 'n_estimators': 130.0}\n"
     ]
    }
   ],
   "source": [
    "# начинаем подбор гиперпараметров\n",
    "\n",
    "trials = Trials() # используется для логирования результатов\n",
    "\n",
    "best=fmin(hyperopt_rf, # наша функция \n",
    "          space=space, # пространство гиперпараметров\n",
    "          algo=tpe.suggest, # алгоритм оптимизации, установлен по умолчанию, задавать необязательно\n",
    "          max_evals=20, # максимальное количество итераций\n",
    "          trials=trials, # логирование результатов\n",
    "          rstate=np.random.default_rng(random_state) # фиксируем для повторяемости результата\n",
    "         )\n",
    "print(\"Наилучшие значения гиперпараметров {}\".format(best))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "26858f82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_score на тестовом наборе: 0.80\n"
     ]
    }
   ],
   "source": [
    "model = ensemble.RandomForestClassifier(\n",
    "    random_state=random_state, \n",
    "    n_estimators=int(best['n_estimators']),\n",
    "    max_depth=int(best['max_depth']),\n",
    "    min_samples_leaf=int(best['min_samples_leaf'])\n",
    ")\n",
    "model.fit(X_train, y_train)\n",
    "y_train_pred = model.predict(X_train)\n",
    "y_test_pred = model.predict(X_test)\n",
    "print('f1_score на тестовом наборе: {:.2f}'.format(metrics.f1_score(y_test, y_test_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "3b970950",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [01:15<00:00,  5.05s/trial, best loss: -0.8196097806854453]\n",
      "Наилучшее значение гиперпараметров: {'max_depth': 18.0, 'min_samples_leaf': 2.0, 'n_estimators': 230.0}\n"
     ]
    }
   ],
   "source": [
    "best=fmin(hyperopt_rf, # наша функция\n",
    "          space=space, # пространство гиперпараметров\n",
    "          algo=tpe.suggest, # алгоритм оптимизации(по умолчанию, задавать необязательно)\n",
    "          max_evals=40, # максимальное количество итераций\n",
    "          trials=trials, # логирование результатов\n",
    "          rstate=np.random.default_rng(random_state)\n",
    "          )\n",
    "\n",
    "print(\"Наилучшее значение гиперпараметров: {}\".format(best))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "caab3e91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_score на тестовом наборе: 0.80\n"
     ]
    }
   ],
   "source": [
    "model = ensemble.RandomForestClassifier(\n",
    "    random_state=random_state, \n",
    "    n_estimators=int(best['n_estimators']),\n",
    "    max_depth=int(best['max_depth']),\n",
    "    min_samples_leaf=int(best['min_samples_leaf'])\n",
    ")\n",
    "model.fit(X_train, y_train)\n",
    "y_train_pred = model.predict(X_train)\n",
    "y_test_pred = model.predict(X_test)\n",
    "print('f1_score на тестовом наборе: {:.2f}'.format(metrics.f1_score(y_test, y_test_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d0636ed",
   "metadata": {},
   "source": [
    "## <center>4.Модели логистической регрессии и рандомного леса методом Optuna<center>\n",
    "### <center>Логистическая регрессия<center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b2e0d3fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_distributions = [\n",
    "              {'penalty': ['l2', 'none'] , # тип регуляризации\n",
    "              'solver': ['lbfgs', 'sag'], # алгоритм оптимизации\n",
    "               'C': [0.01, 0.1, 0.3, 0.5, 0.7, 0.9, 1]}, # уровень силы регуляризации\n",
    "              \n",
    "              {'penalty': ['l1', 'l2'] ,\n",
    "              'solver': ['liblinear', 'saga'],\n",
    "               'C': [0.01, 0.1, 0.3, 0.5, 0.7, 0.9, 1]}\n",
    "              ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6af5cd72",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state = 42\n",
    "def optuna_lr(trial):\n",
    "    # задаем пространство поиска гиперпараметров\n",
    "    penalty = trial.suggest_categorical('penalty', ['l2'])\n",
    "    solver = trial.suggest_categorical('solver', ['lbfgs', 'sag'])\n",
    "    C = trial.suggest_float('C', 0.01, 1)\n",
    "    \n",
    "    # создаем модель\n",
    "    model = linear_model.LogisticRegression(penalty=penalty,\n",
    "                                            solver=solver,\n",
    "                                            C=C,\n",
    "                                            random_state=random_state)\n",
    "    # обучаем модель\n",
    "    model.fit(X_train, y_train)\n",
    "    score = metrics.f1_score(y_train, model.predict(X_train))\n",
    "    \n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "75be4185",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-25 19:09:13,157] A new study created in memory with name: LogisticRegression\n",
      "c:\\Users\\Smoking Shop\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "[I 2025-10-25 19:09:13,852] Trial 0 finished with value: 0.8874051593323217 and parameters: {'penalty': 'l2', 'solver': 'lbfgs', 'C': 0.6775685728285195}. Best is trial 0 with value: 0.8874051593323217.\n",
      "c:\\Users\\Smoking Shop\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "[I 2025-10-25 19:09:14,442] Trial 1 finished with value: 0.89171974522293 and parameters: {'penalty': 'l2', 'solver': 'lbfgs', 'C': 0.881866080631652}. Best is trial 1 with value: 0.89171974522293.\n",
      "c:\\Users\\Smoking Shop\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "[I 2025-10-25 19:09:15,016] Trial 2 finished with value: 0.8906392002423508 and parameters: {'penalty': 'l2', 'solver': 'lbfgs', 'C': 0.9383774282770003}. Best is trial 1 with value: 0.89171974522293.\n",
      "c:\\Users\\Smoking Shop\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "[I 2025-10-25 19:09:18,939] Trial 3 finished with value: 0.883763277693475 and parameters: {'penalty': 'l2', 'solver': 'sag', 'C': 0.8506919865076443}. Best is trial 1 with value: 0.89171974522293.\n",
      "c:\\Users\\Smoking Shop\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "[I 2025-10-25 19:09:19,539] Trial 4 finished with value: 0.89171974522293 and parameters: {'penalty': 'l2', 'solver': 'lbfgs', 'C': 0.8860538694404849}. Best is trial 1 with value: 0.89171974522293.\n",
      "c:\\Users\\Smoking Shop\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "[I 2025-10-25 19:09:20,140] Trial 5 finished with value: 0.8787602552415679 and parameters: {'penalty': 'l2', 'solver': 'lbfgs', 'C': 0.39663315466516696}. Best is trial 1 with value: 0.89171974522293.\n",
      "c:\\Users\\Smoking Shop\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "[I 2025-10-25 19:09:20,693] Trial 6 finished with value: 0.8913834951456311 and parameters: {'penalty': 'l2', 'solver': 'lbfgs', 'C': 0.9056299648691308}. Best is trial 1 with value: 0.89171974522293.\n",
      "c:\\Users\\Smoking Shop\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "[I 2025-10-25 19:09:24,585] Trial 7 finished with value: 0.8794412389918008 and parameters: {'penalty': 'l2', 'solver': 'sag', 'C': 0.4724574068050348}. Best is trial 1 with value: 0.89171974522293.\n",
      "c:\\Users\\Smoking Shop\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "[I 2025-10-25 19:09:25,243] Trial 8 finished with value: 0.8804611650485437 and parameters: {'penalty': 'l2', 'solver': 'lbfgs', 'C': 0.4632519127698179}. Best is trial 1 with value: 0.89171974522293.\n",
      "c:\\Users\\Smoking Shop\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "[I 2025-10-25 19:09:25,828] Trial 9 finished with value: 0.8727712299788456 and parameters: {'penalty': 'l2', 'solver': 'lbfgs', 'C': 0.270513896306716}. Best is trial 1 with value: 0.89171974522293.\n",
      "[I 2025-10-25 19:09:29,515] Trial 10 finished with value: 0.8250074783128926 and parameters: {'penalty': 'l2', 'solver': 'sag', 'C': 0.011843883861719195}. Best is trial 1 with value: 0.89171974522293.\n",
      "c:\\Users\\Smoking Shop\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "[I 2025-10-25 19:09:30,111] Trial 11 finished with value: 0.8884848484848484 and parameters: {'penalty': 'l2', 'solver': 'lbfgs', 'C': 0.6957307456775144}. Best is trial 1 with value: 0.89171974522293.\n",
      "c:\\Users\\Smoking Shop\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "[I 2025-10-25 19:09:30,683] Trial 12 finished with value: 0.8872727272727273 and parameters: {'penalty': 'l2', 'solver': 'lbfgs', 'C': 0.7154186243775325}. Best is trial 1 with value: 0.89171974522293.\n",
      "c:\\Users\\Smoking Shop\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "[I 2025-10-25 19:09:31,282] Trial 13 finished with value: 0.8926622195269861 and parameters: {'penalty': 'l2', 'solver': 'lbfgs', 'C': 0.9872875271229569}. Best is trial 13 with value: 0.8926622195269861.\n",
      "c:\\Users\\Smoking Shop\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "[I 2025-10-25 19:09:35,042] Trial 14 finished with value: 0.8841019417475728 and parameters: {'penalty': 'l2', 'solver': 'sag', 'C': 0.9847631247929567}. Best is trial 13 with value: 0.8926622195269861.\n",
      "c:\\Users\\Smoking Shop\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "[I 2025-10-25 19:09:35,627] Trial 15 finished with value: 0.8904400606980273 and parameters: {'penalty': 'l2', 'solver': 'lbfgs', 'C': 0.7632766254493633}. Best is trial 13 with value: 0.8926622195269861.\n",
      "c:\\Users\\Smoking Shop\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "[I 2025-10-25 19:09:36,203] Trial 16 finished with value: 0.8868668486502881 and parameters: {'penalty': 'l2', 'solver': 'lbfgs', 'C': 0.5931782382253734}. Best is trial 13 with value: 0.8926622195269861.\n",
      "c:\\Users\\Smoking Shop\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "[I 2025-10-25 19:09:36,771] Trial 17 finished with value: 0.8896300788356579 and parameters: {'penalty': 'l2', 'solver': 'lbfgs', 'C': 0.8173007787958184}. Best is trial 13 with value: 0.8926622195269861.\n",
      "c:\\Users\\Smoking Shop\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "[I 2025-10-25 19:09:40,818] Trial 18 finished with value: 0.8841019417475728 and parameters: {'penalty': 'l2', 'solver': 'sag', 'C': 0.9944713364730117}. Best is trial 13 with value: 0.8926622195269861.\n",
      "c:\\Users\\Smoking Shop\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "[I 2025-10-25 19:09:41,437] Trial 19 finished with value: 0.8861911987860395 and parameters: {'penalty': 'l2', 'solver': 'lbfgs', 'C': 0.5873326100437369}. Best is trial 13 with value: 0.8926622195269861.\n"
     ]
    }
   ],
   "source": [
    "study = optuna.create_study(study_name=\"LogisticRegression\", direction=\"maximize\")\n",
    "# ищем лучшую комбинацию гиперпараметров n_trials раз\n",
    "study.optimize(optuna_lr, n_trials=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d0177caa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Наилучшие значения гиперпараметров: {'penalty': 'l2', 'solver': 'lbfgs', 'C': 0.9872875271229569}\n"
     ]
    }
   ],
   "source": [
    "# выводим результаты на обучающей выборке\n",
    "print(\"Наилучшие значения гиперпараметров: {}\".format(study.best_params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cac42b02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_score на тестовом наборе: 0.78\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Smoking Shop\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "# рассчитаем точность для тестовой выборки\n",
    "model = linear_model.LogisticRegression(**study.best_params, random_state=random_state)\n",
    "model.fit(X_train, y_train)\n",
    "y_train_pred = model.predict(X_train)\n",
    "y_test_pred = model.predict(X_test)\n",
    "print(\"f1_score на тестовом наборе: {:.2f}\".format(metrics.f1_score(y_test, y_test_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "444f508f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Smoking Shop\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "[I 2025-10-25 19:12:05,120] Trial 20 finished with value: 0.8685955394816154 and parameters: {'penalty': 'l2', 'solver': 'lbfgs', 'C': 0.24712262171126664}. Best is trial 13 with value: 0.8926622195269861.\n",
      "c:\\Users\\Smoking Shop\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "[I 2025-10-25 19:12:05,735] Trial 21 finished with value: 0.8894960534304797 and parameters: {'penalty': 'l2', 'solver': 'lbfgs', 'C': 0.8557678391590023}. Best is trial 13 with value: 0.8926622195269861.\n",
      "c:\\Users\\Smoking Shop\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "[I 2025-10-25 19:12:06,347] Trial 22 finished with value: 0.889766170665047 and parameters: {'penalty': 'l2', 'solver': 'lbfgs', 'C': 0.793049589747699}. Best is trial 13 with value: 0.8926622195269861.\n",
      "c:\\Users\\Smoking Shop\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "[I 2025-10-25 19:12:06,947] Trial 23 finished with value: 0.8906439854191981 and parameters: {'penalty': 'l2', 'solver': 'lbfgs', 'C': 0.9083199899894661}. Best is trial 13 with value: 0.8926622195269861.\n",
      "c:\\Users\\Smoking Shop\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "[I 2025-10-25 19:12:07,651] Trial 24 finished with value: 0.8885523924894003 and parameters: {'penalty': 'l2', 'solver': 'lbfgs', 'C': 0.6381101064166382}. Best is trial 13 with value: 0.8926622195269861.\n",
      "c:\\Users\\Smoking Shop\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "[I 2025-10-25 19:12:08,462] Trial 25 finished with value: 0.8913109294580684 and parameters: {'penalty': 'l2', 'solver': 'lbfgs', 'C': 0.8931940306407624}. Best is trial 13 with value: 0.8926622195269861.\n",
      "c:\\Users\\Smoking Shop\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "[I 2025-10-25 19:12:13,121] Trial 26 finished with value: 0.8841019417475728 and parameters: {'penalty': 'l2', 'solver': 'sag', 'C': 0.7543920102217636}. Best is trial 13 with value: 0.8926622195269861.\n",
      "c:\\Users\\Smoking Shop\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "[I 2025-10-25 19:12:13,671] Trial 27 finished with value: 0.8908429351121893 and parameters: {'penalty': 'l2', 'solver': 'lbfgs', 'C': 0.9521198406086998}. Best is trial 13 with value: 0.8926622195269861.\n",
      "c:\\Users\\Smoking Shop\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "[I 2025-10-25 19:12:14,244] Trial 28 finished with value: 0.8903735195870027 and parameters: {'penalty': 'l2', 'solver': 'lbfgs', 'C': 0.8390262047383059}. Best is trial 13 with value: 0.8926622195269861.\n",
      "c:\\Users\\Smoking Shop\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "[I 2025-10-25 19:12:14,824] Trial 29 finished with value: 0.8880121396054628 and parameters: {'penalty': 'l2', 'solver': 'lbfgs', 'C': 0.6620767945358064}. Best is trial 13 with value: 0.8926622195269861.\n",
      "[I 2025-10-25 19:12:15,187] Trial 30 finished with value: 0.8253588516746412 and parameters: {'penalty': 'l2', 'solver': 'lbfgs', 'C': 0.01254924185536277}. Best is trial 13 with value: 0.8926622195269861.\n",
      "c:\\Users\\Smoking Shop\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "[I 2025-10-25 19:12:15,848] Trial 31 finished with value: 0.8921212121212121 and parameters: {'penalty': 'l2', 'solver': 'lbfgs', 'C': 0.897727854996699}. Best is trial 13 with value: 0.8926622195269861.\n",
      "c:\\Users\\Smoking Shop\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "[I 2025-10-25 19:12:16,445] Trial 32 finished with value: 0.8919164396003633 and parameters: {'penalty': 'l2', 'solver': 'lbfgs', 'C': 0.9393544029163403}. Best is trial 13 with value: 0.8926622195269861.\n",
      "c:\\Users\\Smoking Shop\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "[I 2025-10-25 19:12:17,005] Trial 33 finished with value: 0.8921212121212121 and parameters: {'penalty': 'l2', 'solver': 'lbfgs', 'C': 0.9564286589185839}. Best is trial 13 with value: 0.8926622195269861.\n",
      "c:\\Users\\Smoking Shop\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "[I 2025-10-25 19:12:17,574] Trial 34 finished with value: 0.8913834951456311 and parameters: {'penalty': 'l2', 'solver': 'lbfgs', 'C': 0.9937932280283454}. Best is trial 13 with value: 0.8926622195269861.\n",
      "c:\\Users\\Smoking Shop\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "[I 2025-10-25 19:12:18,427] Trial 35 finished with value: 0.8913109294580684 and parameters: {'penalty': 'l2', 'solver': 'lbfgs', 'C': 0.9435361513531833}. Best is trial 13 with value: 0.8926622195269861.\n",
      "c:\\Users\\Smoking Shop\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "[I 2025-10-25 19:12:19,052] Trial 36 finished with value: 0.8921212121212121 and parameters: {'penalty': 'l2', 'solver': 'lbfgs', 'C': 0.9335680440460563}. Best is trial 13 with value: 0.8926622195269861.\n",
      "c:\\Users\\Smoking Shop\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "[I 2025-10-25 19:12:24,250] Trial 37 finished with value: 0.8840315725561627 and parameters: {'penalty': 'l2', 'solver': 'sag', 'C': 0.8113771675767969}. Best is trial 13 with value: 0.8926622195269861.\n",
      "c:\\Users\\Smoking Shop\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "[I 2025-10-25 19:12:24,947] Trial 38 finished with value: 0.8921865536038764 and parameters: {'penalty': 'l2', 'solver': 'lbfgs', 'C': 0.8863857519628069}. Best is trial 13 with value: 0.8926622195269861.\n",
      "c:\\Users\\Smoking Shop\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "[I 2025-10-25 19:12:25,541] Trial 39 finished with value: 0.8883495145631068 and parameters: {'penalty': 'l2', 'solver': 'lbfgs', 'C': 0.7480053271210219}. Best is trial 13 with value: 0.8926622195269861.\n",
      "c:\\Users\\Smoking Shop\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "[I 2025-10-25 19:12:26,189] Trial 40 finished with value: 0.891850954256286 and parameters: {'penalty': 'l2', 'solver': 'lbfgs', 'C': 0.8627695419100753}. Best is trial 13 with value: 0.8926622195269861.\n",
      "c:\\Users\\Smoking Shop\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "[I 2025-10-25 19:12:26,896] Trial 41 finished with value: 0.8914493632504549 and parameters: {'penalty': 'l2', 'solver': 'lbfgs', 'C': 0.9160276600109031}. Best is trial 13 with value: 0.8926622195269861.\n",
      "c:\\Users\\Smoking Shop\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "[I 2025-10-25 19:12:27,514] Trial 42 finished with value: 0.8923916338284329 and parameters: {'penalty': 'l2', 'solver': 'lbfgs', 'C': 0.9987903699495662}. Best is trial 13 with value: 0.8926622195269861.\n",
      "c:\\Users\\Smoking Shop\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "[I 2025-10-25 19:12:28,123] Trial 43 finished with value: 0.8908429351121893 and parameters: {'penalty': 'l2', 'solver': 'lbfgs', 'C': 0.8822126915036302}. Best is trial 13 with value: 0.8926622195269861.\n",
      "c:\\Users\\Smoking Shop\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "[I 2025-10-25 19:12:28,702] Trial 44 finished with value: 0.8925970873786407 and parameters: {'penalty': 'l2', 'solver': 'lbfgs', 'C': 0.9982700395264728}. Best is trial 13 with value: 0.8926622195269861.\n"
     ]
    }
   ],
   "source": [
    "study.optimize(optuna_lr, n_trials=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1efe349f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Наилучшие значения гиперпараметров: {'penalty': 'l2', 'solver': 'lbfgs', 'C': 0.9872875271229569}\n",
      "f1_score на тестовом наборе: 0.78\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Smoking Shop\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "# выводим результаты на обучающей выборке\n",
    "print(\"Наилучшие значения гиперпараметров: {}\".format(study.best_params))\n",
    "# рассчитаем точность для тестовой выборки\n",
    "model = linear_model.LogisticRegression(**study.best_params, random_state=random_state)\n",
    "model.fit(X_train, y_train)\n",
    "y_train_pred = model.predict(X_train)\n",
    "y_test_pred = model.predict(X_test)\n",
    "print(\"f1_score на тестовом наборе: {:.2f}\".format(metrics.f1_score(y_test, y_test_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f050560",
   "metadata": {},
   "source": [
    "### <center>Рандомный лес<center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "11044792",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optuna_rf(trial):\n",
    "    # задаем пространство поиска гиперпараметров\n",
    "    n_estimators = trial.suggest_int('n_estimators', 100, 300, 10)\n",
    "    max_depth = trial.suggest_int('max_depth', 5, 30, 1)\n",
    "    min_samples_leaf = trial.suggest_int('min_samples_leaf', 2, 10, 1)\n",
    "    \n",
    "    # создаем модель\n",
    "    model = ensemble.RandomForestClassifier(n_estimators=n_estimators,\n",
    "                                            max_depth=max_depth,\n",
    "                                            min_samples_leaf=min_samples_leaf,\n",
    "                                            random_state=random_state)\n",
    "    # обучаем модель\n",
    "    model.fit(X_train, y_train)\n",
    "    score = metrics.f1_score(y_train, model.predict(X_train))\n",
    "    \n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "30729355",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-27 12:55:20,218] A new study created in memory with name: RandomForestClassifier\n",
      "C:\\Users\\HYPERPC\\AppData\\Local\\Temp\\ipykernel_19280\\2764605942.py:3: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "Positional arguments ['self', 'name', 'low', 'high', 'step', 'log'] in suggest_int() have been deprecated since v3.5.0. They will be replaced with the corresponding keyword arguments in v5.0.0, so please use the keyword specification instead. See https://github.com/optuna/optuna/releases/tag/v3.5.0 for details.\n",
      "  n_estimators = trial.suggest_int('n_estimators', 100, 300, 10)\n",
      "C:\\Users\\HYPERPC\\AppData\\Local\\Temp\\ipykernel_19280\\2764605942.py:4: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "Positional arguments ['self', 'name', 'low', 'high', 'step', 'log'] in suggest_int() have been deprecated since v3.5.0. They will be replaced with the corresponding keyword arguments in v5.0.0, so please use the keyword specification instead. See https://github.com/optuna/optuna/releases/tag/v3.5.0 for details.\n",
      "  max_depth = trial.suggest_int('max_depth', 5, 30, 1)\n",
      "C:\\Users\\HYPERPC\\AppData\\Local\\Temp\\ipykernel_19280\\2764605942.py:5: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "Positional arguments ['self', 'name', 'low', 'high', 'step', 'log'] in suggest_int() have been deprecated since v3.5.0. They will be replaced with the corresponding keyword arguments in v5.0.0, so please use the keyword specification instead. See https://github.com/optuna/optuna/releases/tag/v3.5.0 for details.\n",
      "  min_samples_leaf = trial.suggest_int('min_samples_leaf', 2, 10, 1)\n",
      "[I 2025-10-27 12:55:21,907] Trial 0 finished with value: 0.9880551301684533 and parameters: {'n_estimators': 170, 'max_depth': 20, 'min_samples_leaf': 2}. Best is trial 0 with value: 0.9880551301684533.\n",
      "C:\\Users\\HYPERPC\\AppData\\Local\\Temp\\ipykernel_19280\\2764605942.py:3: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "Positional arguments ['self', 'name', 'low', 'high', 'step', 'log'] in suggest_int() have been deprecated since v3.5.0. They will be replaced with the corresponding keyword arguments in v5.0.0, so please use the keyword specification instead. See https://github.com/optuna/optuna/releases/tag/v3.5.0 for details.\n",
      "  n_estimators = trial.suggest_int('n_estimators', 100, 300, 10)\n",
      "C:\\Users\\HYPERPC\\AppData\\Local\\Temp\\ipykernel_19280\\2764605942.py:4: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "Positional arguments ['self', 'name', 'low', 'high', 'step', 'log'] in suggest_int() have been deprecated since v3.5.0. They will be replaced with the corresponding keyword arguments in v5.0.0, so please use the keyword specification instead. See https://github.com/optuna/optuna/releases/tag/v3.5.0 for details.\n",
      "  max_depth = trial.suggest_int('max_depth', 5, 30, 1)\n",
      "C:\\Users\\HYPERPC\\AppData\\Local\\Temp\\ipykernel_19280\\2764605942.py:5: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "Positional arguments ['self', 'name', 'low', 'high', 'step', 'log'] in suggest_int() have been deprecated since v3.5.0. They will be replaced with the corresponding keyword arguments in v5.0.0, so please use the keyword specification instead. See https://github.com/optuna/optuna/releases/tag/v3.5.0 for details.\n",
      "  min_samples_leaf = trial.suggest_int('min_samples_leaf', 2, 10, 1)\n",
      "[I 2025-10-27 12:55:23,211] Trial 1 finished with value: 0.921580547112462 and parameters: {'n_estimators': 170, 'max_depth': 23, 'min_samples_leaf': 7}. Best is trial 0 with value: 0.9880551301684533.\n",
      "C:\\Users\\HYPERPC\\AppData\\Local\\Temp\\ipykernel_19280\\2764605942.py:3: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "Positional arguments ['self', 'name', 'low', 'high', 'step', 'log'] in suggest_int() have been deprecated since v3.5.0. They will be replaced with the corresponding keyword arguments in v5.0.0, so please use the keyword specification instead. See https://github.com/optuna/optuna/releases/tag/v3.5.0 for details.\n",
      "  n_estimators = trial.suggest_int('n_estimators', 100, 300, 10)\n",
      "C:\\Users\\HYPERPC\\AppData\\Local\\Temp\\ipykernel_19280\\2764605942.py:4: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "Positional arguments ['self', 'name', 'low', 'high', 'step', 'log'] in suggest_int() have been deprecated since v3.5.0. They will be replaced with the corresponding keyword arguments in v5.0.0, so please use the keyword specification instead. See https://github.com/optuna/optuna/releases/tag/v3.5.0 for details.\n",
      "  max_depth = trial.suggest_int('max_depth', 5, 30, 1)\n",
      "C:\\Users\\HYPERPC\\AppData\\Local\\Temp\\ipykernel_19280\\2764605942.py:5: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "Positional arguments ['self', 'name', 'low', 'high', 'step', 'log'] in suggest_int() have been deprecated since v3.5.0. They will be replaced with the corresponding keyword arguments in v5.0.0, so please use the keyword specification instead. See https://github.com/optuna/optuna/releases/tag/v3.5.0 for details.\n",
      "  min_samples_leaf = trial.suggest_int('min_samples_leaf', 2, 10, 1)\n",
      "[I 2025-10-27 12:55:24,947] Trial 2 finished with value: 0.8959028831562974 and parameters: {'n_estimators': 250, 'max_depth': 20, 'min_samples_leaf': 10}. Best is trial 0 with value: 0.9880551301684533.\n",
      "C:\\Users\\HYPERPC\\AppData\\Local\\Temp\\ipykernel_19280\\2764605942.py:3: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "Positional arguments ['self', 'name', 'low', 'high', 'step', 'log'] in suggest_int() have been deprecated since v3.5.0. They will be replaced with the corresponding keyword arguments in v5.0.0, so please use the keyword specification instead. See https://github.com/optuna/optuna/releases/tag/v3.5.0 for details.\n",
      "  n_estimators = trial.suggest_int('n_estimators', 100, 300, 10)\n",
      "C:\\Users\\HYPERPC\\AppData\\Local\\Temp\\ipykernel_19280\\2764605942.py:4: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "Positional arguments ['self', 'name', 'low', 'high', 'step', 'log'] in suggest_int() have been deprecated since v3.5.0. They will be replaced with the corresponding keyword arguments in v5.0.0, so please use the keyword specification instead. See https://github.com/optuna/optuna/releases/tag/v3.5.0 for details.\n",
      "  max_depth = trial.suggest_int('max_depth', 5, 30, 1)\n",
      "C:\\Users\\HYPERPC\\AppData\\Local\\Temp\\ipykernel_19280\\2764605942.py:5: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "Positional arguments ['self', 'name', 'low', 'high', 'step', 'log'] in suggest_int() have been deprecated since v3.5.0. They will be replaced with the corresponding keyword arguments in v5.0.0, so please use the keyword specification instead. See https://github.com/optuna/optuna/releases/tag/v3.5.0 for details.\n",
      "  min_samples_leaf = trial.suggest_int('min_samples_leaf', 2, 10, 1)\n",
      "[I 2025-10-27 12:55:26,058] Trial 3 finished with value: 0.8678899082568807 and parameters: {'n_estimators': 210, 'max_depth': 7, 'min_samples_leaf': 3}. Best is trial 0 with value: 0.9880551301684533.\n",
      "C:\\Users\\HYPERPC\\AppData\\Local\\Temp\\ipykernel_19280\\2764605942.py:3: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "Positional arguments ['self', 'name', 'low', 'high', 'step', 'log'] in suggest_int() have been deprecated since v3.5.0. They will be replaced with the corresponding keyword arguments in v5.0.0, so please use the keyword specification instead. See https://github.com/optuna/optuna/releases/tag/v3.5.0 for details.\n",
      "  n_estimators = trial.suggest_int('n_estimators', 100, 300, 10)\n",
      "C:\\Users\\HYPERPC\\AppData\\Local\\Temp\\ipykernel_19280\\2764605942.py:4: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "Positional arguments ['self', 'name', 'low', 'high', 'step', 'log'] in suggest_int() have been deprecated since v3.5.0. They will be replaced with the corresponding keyword arguments in v5.0.0, so please use the keyword specification instead. See https://github.com/optuna/optuna/releases/tag/v3.5.0 for details.\n",
      "  max_depth = trial.suggest_int('max_depth', 5, 30, 1)\n",
      "C:\\Users\\HYPERPC\\AppData\\Local\\Temp\\ipykernel_19280\\2764605942.py:5: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "Positional arguments ['self', 'name', 'low', 'high', 'step', 'log'] in suggest_int() have been deprecated since v3.5.0. They will be replaced with the corresponding keyword arguments in v5.0.0, so please use the keyword specification instead. See https://github.com/optuna/optuna/releases/tag/v3.5.0 for details.\n",
      "  min_samples_leaf = trial.suggest_int('min_samples_leaf', 2, 10, 1)\n",
      "[I 2025-10-27 12:55:28,208] Trial 4 finished with value: 0.9892736745326387 and parameters: {'n_estimators': 220, 'max_depth': 20, 'min_samples_leaf': 2}. Best is trial 4 with value: 0.9892736745326387.\n",
      "C:\\Users\\HYPERPC\\AppData\\Local\\Temp\\ipykernel_19280\\2764605942.py:3: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "Positional arguments ['self', 'name', 'low', 'high', 'step', 'log'] in suggest_int() have been deprecated since v3.5.0. They will be replaced with the corresponding keyword arguments in v5.0.0, so please use the keyword specification instead. See https://github.com/optuna/optuna/releases/tag/v3.5.0 for details.\n",
      "  n_estimators = trial.suggest_int('n_estimators', 100, 300, 10)\n",
      "C:\\Users\\HYPERPC\\AppData\\Local\\Temp\\ipykernel_19280\\2764605942.py:4: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "Positional arguments ['self', 'name', 'low', 'high', 'step', 'log'] in suggest_int() have been deprecated since v3.5.0. They will be replaced with the corresponding keyword arguments in v5.0.0, so please use the keyword specification instead. See https://github.com/optuna/optuna/releases/tag/v3.5.0 for details.\n",
      "  max_depth = trial.suggest_int('max_depth', 5, 30, 1)\n",
      "C:\\Users\\HYPERPC\\AppData\\Local\\Temp\\ipykernel_19280\\2764605942.py:5: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "Positional arguments ['self', 'name', 'low', 'high', 'step', 'log'] in suggest_int() have been deprecated since v3.5.0. They will be replaced with the corresponding keyword arguments in v5.0.0, so please use the keyword specification instead. See https://github.com/optuna/optuna/releases/tag/v3.5.0 for details.\n",
      "  min_samples_leaf = trial.suggest_int('min_samples_leaf', 2, 10, 1)\n",
      "[I 2025-10-27 12:55:29,358] Trial 5 finished with value: 0.904126213592233 and parameters: {'n_estimators': 160, 'max_depth': 29, 'min_samples_leaf': 9}. Best is trial 4 with value: 0.9892736745326387.\n",
      "C:\\Users\\HYPERPC\\AppData\\Local\\Temp\\ipykernel_19280\\2764605942.py:3: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "Positional arguments ['self', 'name', 'low', 'high', 'step', 'log'] in suggest_int() have been deprecated since v3.5.0. They will be replaced with the corresponding keyword arguments in v5.0.0, so please use the keyword specification instead. See https://github.com/optuna/optuna/releases/tag/v3.5.0 for details.\n",
      "  n_estimators = trial.suggest_int('n_estimators', 100, 300, 10)\n",
      "C:\\Users\\HYPERPC\\AppData\\Local\\Temp\\ipykernel_19280\\2764605942.py:4: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "Positional arguments ['self', 'name', 'low', 'high', 'step', 'log'] in suggest_int() have been deprecated since v3.5.0. They will be replaced with the corresponding keyword arguments in v5.0.0, so please use the keyword specification instead. See https://github.com/optuna/optuna/releases/tag/v3.5.0 for details.\n",
      "  max_depth = trial.suggest_int('max_depth', 5, 30, 1)\n",
      "C:\\Users\\HYPERPC\\AppData\\Local\\Temp\\ipykernel_19280\\2764605942.py:5: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "Positional arguments ['self', 'name', 'low', 'high', 'step', 'log'] in suggest_int() have been deprecated since v3.5.0. They will be replaced with the corresponding keyword arguments in v5.0.0, so please use the keyword specification instead. See https://github.com/optuna/optuna/releases/tag/v3.5.0 for details.\n",
      "  min_samples_leaf = trial.suggest_int('min_samples_leaf', 2, 10, 1)\n",
      "[I 2025-10-27 12:55:31,188] Trial 6 finished with value: 0.903028449066993 and parameters: {'n_estimators': 270, 'max_depth': 11, 'min_samples_leaf': 7}. Best is trial 4 with value: 0.9892736745326387.\n",
      "C:\\Users\\HYPERPC\\AppData\\Local\\Temp\\ipykernel_19280\\2764605942.py:3: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "Positional arguments ['self', 'name', 'low', 'high', 'step', 'log'] in suggest_int() have been deprecated since v3.5.0. They will be replaced with the corresponding keyword arguments in v5.0.0, so please use the keyword specification instead. See https://github.com/optuna/optuna/releases/tag/v3.5.0 for details.\n",
      "  n_estimators = trial.suggest_int('n_estimators', 100, 300, 10)\n",
      "C:\\Users\\HYPERPC\\AppData\\Local\\Temp\\ipykernel_19280\\2764605942.py:4: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "Positional arguments ['self', 'name', 'low', 'high', 'step', 'log'] in suggest_int() have been deprecated since v3.5.0. They will be replaced with the corresponding keyword arguments in v5.0.0, so please use the keyword specification instead. See https://github.com/optuna/optuna/releases/tag/v3.5.0 for details.\n",
      "  max_depth = trial.suggest_int('max_depth', 5, 30, 1)\n",
      "C:\\Users\\HYPERPC\\AppData\\Local\\Temp\\ipykernel_19280\\2764605942.py:5: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "Positional arguments ['self', 'name', 'low', 'high', 'step', 'log'] in suggest_int() have been deprecated since v3.5.0. They will be replaced with the corresponding keyword arguments in v5.0.0, so please use the keyword specification instead. See https://github.com/optuna/optuna/releases/tag/v3.5.0 for details.\n",
      "  min_samples_leaf = trial.suggest_int('min_samples_leaf', 2, 10, 1)\n",
      "[I 2025-10-27 12:55:31,993] Trial 7 finished with value: 0.8333846627656298 and parameters: {'n_estimators': 180, 'max_depth': 6, 'min_samples_leaf': 6}. Best is trial 4 with value: 0.9892736745326387.\n",
      "C:\\Users\\HYPERPC\\AppData\\Local\\Temp\\ipykernel_19280\\2764605942.py:3: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "Positional arguments ['self', 'name', 'low', 'high', 'step', 'log'] in suggest_int() have been deprecated since v3.5.0. They will be replaced with the corresponding keyword arguments in v5.0.0, so please use the keyword specification instead. See https://github.com/optuna/optuna/releases/tag/v3.5.0 for details.\n",
      "  n_estimators = trial.suggest_int('n_estimators', 100, 300, 10)\n",
      "C:\\Users\\HYPERPC\\AppData\\Local\\Temp\\ipykernel_19280\\2764605942.py:4: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "Positional arguments ['self', 'name', 'low', 'high', 'step', 'log'] in suggest_int() have been deprecated since v3.5.0. They will be replaced with the corresponding keyword arguments in v5.0.0, so please use the keyword specification instead. See https://github.com/optuna/optuna/releases/tag/v3.5.0 for details.\n",
      "  max_depth = trial.suggest_int('max_depth', 5, 30, 1)\n",
      "C:\\Users\\HYPERPC\\AppData\\Local\\Temp\\ipykernel_19280\\2764605942.py:5: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "Positional arguments ['self', 'name', 'low', 'high', 'step', 'log'] in suggest_int() have been deprecated since v3.5.0. They will be replaced with the corresponding keyword arguments in v5.0.0, so please use the keyword specification instead. See https://github.com/optuna/optuna/releases/tag/v3.5.0 for details.\n",
      "  min_samples_leaf = trial.suggest_int('min_samples_leaf', 2, 10, 1)\n",
      "[I 2025-10-27 12:55:34,699] Trial 8 finished with value: 0.9920245398773007 and parameters: {'n_estimators': 280, 'max_depth': 28, 'min_samples_leaf': 2}. Best is trial 8 with value: 0.9920245398773007.\n",
      "C:\\Users\\HYPERPC\\AppData\\Local\\Temp\\ipykernel_19280\\2764605942.py:3: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "Positional arguments ['self', 'name', 'low', 'high', 'step', 'log'] in suggest_int() have been deprecated since v3.5.0. They will be replaced with the corresponding keyword arguments in v5.0.0, so please use the keyword specification instead. See https://github.com/optuna/optuna/releases/tag/v3.5.0 for details.\n",
      "  n_estimators = trial.suggest_int('n_estimators', 100, 300, 10)\n",
      "C:\\Users\\HYPERPC\\AppData\\Local\\Temp\\ipykernel_19280\\2764605942.py:4: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "Positional arguments ['self', 'name', 'low', 'high', 'step', 'log'] in suggest_int() have been deprecated since v3.5.0. They will be replaced with the corresponding keyword arguments in v5.0.0, so please use the keyword specification instead. See https://github.com/optuna/optuna/releases/tag/v3.5.0 for details.\n",
      "  max_depth = trial.suggest_int('max_depth', 5, 30, 1)\n",
      "C:\\Users\\HYPERPC\\AppData\\Local\\Temp\\ipykernel_19280\\2764605942.py:5: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "Positional arguments ['self', 'name', 'low', 'high', 'step', 'log'] in suggest_int() have been deprecated since v3.5.0. They will be replaced with the corresponding keyword arguments in v5.0.0, so please use the keyword specification instead. See https://github.com/optuna/optuna/releases/tag/v3.5.0 for details.\n",
      "  min_samples_leaf = trial.suggest_int('min_samples_leaf', 2, 10, 1)\n",
      "[I 2025-10-27 12:55:35,655] Trial 9 finished with value: 0.923919659160073 and parameters: {'n_estimators': 120, 'max_depth': 15, 'min_samples_leaf': 6}. Best is trial 8 with value: 0.9920245398773007.\n",
      "C:\\Users\\HYPERPC\\AppData\\Local\\Temp\\ipykernel_19280\\2764605942.py:3: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "Positional arguments ['self', 'name', 'low', 'high', 'step', 'log'] in suggest_int() have been deprecated since v3.5.0. They will be replaced with the corresponding keyword arguments in v5.0.0, so please use the keyword specification instead. See https://github.com/optuna/optuna/releases/tag/v3.5.0 for details.\n",
      "  n_estimators = trial.suggest_int('n_estimators', 100, 300, 10)\n",
      "C:\\Users\\HYPERPC\\AppData\\Local\\Temp\\ipykernel_19280\\2764605942.py:4: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "Positional arguments ['self', 'name', 'low', 'high', 'step', 'log'] in suggest_int() have been deprecated since v3.5.0. They will be replaced with the corresponding keyword arguments in v5.0.0, so please use the keyword specification instead. See https://github.com/optuna/optuna/releases/tag/v3.5.0 for details.\n",
      "  max_depth = trial.suggest_int('max_depth', 5, 30, 1)\n",
      "C:\\Users\\HYPERPC\\AppData\\Local\\Temp\\ipykernel_19280\\2764605942.py:5: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "Positional arguments ['self', 'name', 'low', 'high', 'step', 'log'] in suggest_int() have been deprecated since v3.5.0. They will be replaced with the corresponding keyword arguments in v5.0.0, so please use the keyword specification instead. See https://github.com/optuna/optuna/releases/tag/v3.5.0 for details.\n",
      "  min_samples_leaf = trial.suggest_int('min_samples_leaf', 2, 10, 1)\n",
      "[I 2025-10-27 12:55:38,238] Trial 10 finished with value: 0.9585870889159561 and parameters: {'n_estimators': 300, 'max_depth': 30, 'min_samples_leaf': 4}. Best is trial 8 with value: 0.9920245398773007.\n",
      "C:\\Users\\HYPERPC\\AppData\\Local\\Temp\\ipykernel_19280\\2764605942.py:3: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "Positional arguments ['self', 'name', 'low', 'high', 'step', 'log'] in suggest_int() have been deprecated since v3.5.0. They will be replaced with the corresponding keyword arguments in v5.0.0, so please use the keyword specification instead. See https://github.com/optuna/optuna/releases/tag/v3.5.0 for details.\n",
      "  n_estimators = trial.suggest_int('n_estimators', 100, 300, 10)\n",
      "C:\\Users\\HYPERPC\\AppData\\Local\\Temp\\ipykernel_19280\\2764605942.py:4: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "Positional arguments ['self', 'name', 'low', 'high', 'step', 'log'] in suggest_int() have been deprecated since v3.5.0. They will be replaced with the corresponding keyword arguments in v5.0.0, so please use the keyword specification instead. See https://github.com/optuna/optuna/releases/tag/v3.5.0 for details.\n",
      "  max_depth = trial.suggest_int('max_depth', 5, 30, 1)\n",
      "C:\\Users\\HYPERPC\\AppData\\Local\\Temp\\ipykernel_19280\\2764605942.py:5: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "Positional arguments ['self', 'name', 'low', 'high', 'step', 'log'] in suggest_int() have been deprecated since v3.5.0. They will be replaced with the corresponding keyword arguments in v5.0.0, so please use the keyword specification instead. See https://github.com/optuna/optuna/releases/tag/v3.5.0 for details.\n",
      "  min_samples_leaf = trial.suggest_int('min_samples_leaf', 2, 10, 1)\n",
      "[I 2025-10-27 12:55:40,566] Trial 11 finished with value: 0.9911070223857712 and parameters: {'n_estimators': 230, 'max_depth': 25, 'min_samples_leaf': 2}. Best is trial 8 with value: 0.9920245398773007.\n",
      "C:\\Users\\HYPERPC\\AppData\\Local\\Temp\\ipykernel_19280\\2764605942.py:3: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "Positional arguments ['self', 'name', 'low', 'high', 'step', 'log'] in suggest_int() have been deprecated since v3.5.0. They will be replaced with the corresponding keyword arguments in v5.0.0, so please use the keyword specification instead. See https://github.com/optuna/optuna/releases/tag/v3.5.0 for details.\n",
      "  n_estimators = trial.suggest_int('n_estimators', 100, 300, 10)\n",
      "C:\\Users\\HYPERPC\\AppData\\Local\\Temp\\ipykernel_19280\\2764605942.py:4: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "Positional arguments ['self', 'name', 'low', 'high', 'step', 'log'] in suggest_int() have been deprecated since v3.5.0. They will be replaced with the corresponding keyword arguments in v5.0.0, so please use the keyword specification instead. See https://github.com/optuna/optuna/releases/tag/v3.5.0 for details.\n",
      "  max_depth = trial.suggest_int('max_depth', 5, 30, 1)\n",
      "C:\\Users\\HYPERPC\\AppData\\Local\\Temp\\ipykernel_19280\\2764605942.py:5: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "Positional arguments ['self', 'name', 'low', 'high', 'step', 'log'] in suggest_int() have been deprecated since v3.5.0. They will be replaced with the corresponding keyword arguments in v5.0.0, so please use the keyword specification instead. See https://github.com/optuna/optuna/releases/tag/v3.5.0 for details.\n",
      "  min_samples_leaf = trial.suggest_int('min_samples_leaf', 2, 10, 1)\n",
      "[I 2025-10-27 12:55:42,766] Trial 12 finished with value: 0.9582952815829529 and parameters: {'n_estimators': 250, 'max_depth': 26, 'min_samples_leaf': 4}. Best is trial 8 with value: 0.9920245398773007.\n",
      "C:\\Users\\HYPERPC\\AppData\\Local\\Temp\\ipykernel_19280\\2764605942.py:3: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "Positional arguments ['self', 'name', 'low', 'high', 'step', 'log'] in suggest_int() have been deprecated since v3.5.0. They will be replaced with the corresponding keyword arguments in v5.0.0, so please use the keyword specification instead. See https://github.com/optuna/optuna/releases/tag/v3.5.0 for details.\n",
      "  n_estimators = trial.suggest_int('n_estimators', 100, 300, 10)\n",
      "C:\\Users\\HYPERPC\\AppData\\Local\\Temp\\ipykernel_19280\\2764605942.py:4: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "Positional arguments ['self', 'name', 'low', 'high', 'step', 'log'] in suggest_int() have been deprecated since v3.5.0. They will be replaced with the corresponding keyword arguments in v5.0.0, so please use the keyword specification instead. See https://github.com/optuna/optuna/releases/tag/v3.5.0 for details.\n",
      "  max_depth = trial.suggest_int('max_depth', 5, 30, 1)\n",
      "C:\\Users\\HYPERPC\\AppData\\Local\\Temp\\ipykernel_19280\\2764605942.py:5: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "Positional arguments ['self', 'name', 'low', 'high', 'step', 'log'] in suggest_int() have been deprecated since v3.5.0. They will be replaced with the corresponding keyword arguments in v5.0.0, so please use the keyword specification instead. See https://github.com/optuna/optuna/releases/tag/v3.5.0 for details.\n",
      "  min_samples_leaf = trial.suggest_int('min_samples_leaf', 2, 10, 1)\n",
      "[I 2025-10-27 12:55:45,347] Trial 13 finished with value: 0.9582952815829529 and parameters: {'n_estimators': 300, 'max_depth': 25, 'min_samples_leaf': 4}. Best is trial 8 with value: 0.9920245398773007.\n",
      "C:\\Users\\HYPERPC\\AppData\\Local\\Temp\\ipykernel_19280\\2764605942.py:3: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "Positional arguments ['self', 'name', 'low', 'high', 'step', 'log'] in suggest_int() have been deprecated since v3.5.0. They will be replaced with the corresponding keyword arguments in v5.0.0, so please use the keyword specification instead. See https://github.com/optuna/optuna/releases/tag/v3.5.0 for details.\n",
      "  n_estimators = trial.suggest_int('n_estimators', 100, 300, 10)\n",
      "C:\\Users\\HYPERPC\\AppData\\Local\\Temp\\ipykernel_19280\\2764605942.py:4: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "Positional arguments ['self', 'name', 'low', 'high', 'step', 'log'] in suggest_int() have been deprecated since v3.5.0. They will be replaced with the corresponding keyword arguments in v5.0.0, so please use the keyword specification instead. See https://github.com/optuna/optuna/releases/tag/v3.5.0 for details.\n",
      "  max_depth = trial.suggest_int('max_depth', 5, 30, 1)\n",
      "C:\\Users\\HYPERPC\\AppData\\Local\\Temp\\ipykernel_19280\\2764605942.py:5: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "Positional arguments ['self', 'name', 'low', 'high', 'step', 'log'] in suggest_int() have been deprecated since v3.5.0. They will be replaced with the corresponding keyword arguments in v5.0.0, so please use the keyword specification instead. See https://github.com/optuna/optuna/releases/tag/v3.5.0 for details.\n",
      "  min_samples_leaf = trial.suggest_int('min_samples_leaf', 2, 10, 1)\n",
      "[I 2025-10-27 12:55:47,731] Trial 14 finished with value: 0.9911015648972077 and parameters: {'n_estimators': 240, 'max_depth': 27, 'min_samples_leaf': 2}. Best is trial 8 with value: 0.9920245398773007.\n",
      "C:\\Users\\HYPERPC\\AppData\\Local\\Temp\\ipykernel_19280\\2764605942.py:3: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "Positional arguments ['self', 'name', 'low', 'high', 'step', 'log'] in suggest_int() have been deprecated since v3.5.0. They will be replaced with the corresponding keyword arguments in v5.0.0, so please use the keyword specification instead. See https://github.com/optuna/optuna/releases/tag/v3.5.0 for details.\n",
      "  n_estimators = trial.suggest_int('n_estimators', 100, 300, 10)\n",
      "C:\\Users\\HYPERPC\\AppData\\Local\\Temp\\ipykernel_19280\\2764605942.py:4: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "Positional arguments ['self', 'name', 'low', 'high', 'step', 'log'] in suggest_int() have been deprecated since v3.5.0. They will be replaced with the corresponding keyword arguments in v5.0.0, so please use the keyword specification instead. See https://github.com/optuna/optuna/releases/tag/v3.5.0 for details.\n",
      "  max_depth = trial.suggest_int('max_depth', 5, 30, 1)\n",
      "C:\\Users\\HYPERPC\\AppData\\Local\\Temp\\ipykernel_19280\\2764605942.py:5: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "Positional arguments ['self', 'name', 'low', 'high', 'step', 'log'] in suggest_int() have been deprecated since v3.5.0. They will be replaced with the corresponding keyword arguments in v5.0.0, so please use the keyword specification instead. See https://github.com/optuna/optuna/releases/tag/v3.5.0 for details.\n",
      "  min_samples_leaf = trial.suggest_int('min_samples_leaf', 2, 10, 1)\n",
      "[I 2025-10-27 12:55:50,169] Trial 15 finished with value: 0.9740932642487047 and parameters: {'n_estimators': 270, 'max_depth': 23, 'min_samples_leaf': 3}. Best is trial 8 with value: 0.9920245398773007.\n",
      "C:\\Users\\HYPERPC\\AppData\\Local\\Temp\\ipykernel_19280\\2764605942.py:3: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "Positional arguments ['self', 'name', 'low', 'high', 'step', 'log'] in suggest_int() have been deprecated since v3.5.0. They will be replaced with the corresponding keyword arguments in v5.0.0, so please use the keyword specification instead. See https://github.com/optuna/optuna/releases/tag/v3.5.0 for details.\n",
      "  n_estimators = trial.suggest_int('n_estimators', 100, 300, 10)\n",
      "C:\\Users\\HYPERPC\\AppData\\Local\\Temp\\ipykernel_19280\\2764605942.py:4: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "Positional arguments ['self', 'name', 'low', 'high', 'step', 'log'] in suggest_int() have been deprecated since v3.5.0. They will be replaced with the corresponding keyword arguments in v5.0.0, so please use the keyword specification instead. See https://github.com/optuna/optuna/releases/tag/v3.5.0 for details.\n",
      "  max_depth = trial.suggest_int('max_depth', 5, 30, 1)\n",
      "C:\\Users\\HYPERPC\\AppData\\Local\\Temp\\ipykernel_19280\\2764605942.py:5: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "Positional arguments ['self', 'name', 'low', 'high', 'step', 'log'] in suggest_int() have been deprecated since v3.5.0. They will be replaced with the corresponding keyword arguments in v5.0.0, so please use the keyword specification instead. See https://github.com/optuna/optuna/releases/tag/v3.5.0 for details.\n",
      "  min_samples_leaf = trial.suggest_int('min_samples_leaf', 2, 10, 1)\n",
      "[I 2025-10-27 12:55:52,412] Trial 16 finished with value: 0.9407114624505929 and parameters: {'n_estimators': 280, 'max_depth': 16, 'min_samples_leaf': 5}. Best is trial 8 with value: 0.9920245398773007.\n",
      "C:\\Users\\HYPERPC\\AppData\\Local\\Temp\\ipykernel_19280\\2764605942.py:3: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "Positional arguments ['self', 'name', 'low', 'high', 'step', 'log'] in suggest_int() have been deprecated since v3.5.0. They will be replaced with the corresponding keyword arguments in v5.0.0, so please use the keyword specification instead. See https://github.com/optuna/optuna/releases/tag/v3.5.0 for details.\n",
      "  n_estimators = trial.suggest_int('n_estimators', 100, 300, 10)\n",
      "C:\\Users\\HYPERPC\\AppData\\Local\\Temp\\ipykernel_19280\\2764605942.py:4: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "Positional arguments ['self', 'name', 'low', 'high', 'step', 'log'] in suggest_int() have been deprecated since v3.5.0. They will be replaced with the corresponding keyword arguments in v5.0.0, so please use the keyword specification instead. See https://github.com/optuna/optuna/releases/tag/v3.5.0 for details.\n",
      "  max_depth = trial.suggest_int('max_depth', 5, 30, 1)\n",
      "C:\\Users\\HYPERPC\\AppData\\Local\\Temp\\ipykernel_19280\\2764605942.py:5: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "Positional arguments ['self', 'name', 'low', 'high', 'step', 'log'] in suggest_int() have been deprecated since v3.5.0. They will be replaced with the corresponding keyword arguments in v5.0.0, so please use the keyword specification instead. See https://github.com/optuna/optuna/releases/tag/v3.5.0 for details.\n",
      "  min_samples_leaf = trial.suggest_int('min_samples_leaf', 2, 10, 1)\n",
      "[I 2025-10-27 12:55:54,523] Trial 17 finished with value: 0.9734675205855444 and parameters: {'n_estimators': 230, 'max_depth': 30, 'min_samples_leaf': 3}. Best is trial 8 with value: 0.9920245398773007.\n",
      "C:\\Users\\HYPERPC\\AppData\\Local\\Temp\\ipykernel_19280\\2764605942.py:3: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "Positional arguments ['self', 'name', 'low', 'high', 'step', 'log'] in suggest_int() have been deprecated since v3.5.0. They will be replaced with the corresponding keyword arguments in v5.0.0, so please use the keyword specification instead. See https://github.com/optuna/optuna/releases/tag/v3.5.0 for details.\n",
      "  n_estimators = trial.suggest_int('n_estimators', 100, 300, 10)\n",
      "C:\\Users\\HYPERPC\\AppData\\Local\\Temp\\ipykernel_19280\\2764605942.py:4: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "Positional arguments ['self', 'name', 'low', 'high', 'step', 'log'] in suggest_int() have been deprecated since v3.5.0. They will be replaced with the corresponding keyword arguments in v5.0.0, so please use the keyword specification instead. See https://github.com/optuna/optuna/releases/tag/v3.5.0 for details.\n",
      "  max_depth = trial.suggest_int('max_depth', 5, 30, 1)\n",
      "C:\\Users\\HYPERPC\\AppData\\Local\\Temp\\ipykernel_19280\\2764605942.py:5: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "Positional arguments ['self', 'name', 'low', 'high', 'step', 'log'] in suggest_int() have been deprecated since v3.5.0. They will be replaced with the corresponding keyword arguments in v5.0.0, so please use the keyword specification instead. See https://github.com/optuna/optuna/releases/tag/v3.5.0 for details.\n",
      "  min_samples_leaf = trial.suggest_int('min_samples_leaf', 2, 10, 1)\n",
      "[I 2025-10-27 12:55:55,594] Trial 18 finished with value: 0.9414276998169616 and parameters: {'n_estimators': 130, 'max_depth': 24, 'min_samples_leaf': 5}. Best is trial 8 with value: 0.9920245398773007.\n",
      "C:\\Users\\HYPERPC\\AppData\\Local\\Temp\\ipykernel_19280\\2764605942.py:3: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "Positional arguments ['self', 'name', 'low', 'high', 'step', 'log'] in suggest_int() have been deprecated since v3.5.0. They will be replaced with the corresponding keyword arguments in v5.0.0, so please use the keyword specification instead. See https://github.com/optuna/optuna/releases/tag/v3.5.0 for details.\n",
      "  n_estimators = trial.suggest_int('n_estimators', 100, 300, 10)\n",
      "C:\\Users\\HYPERPC\\AppData\\Local\\Temp\\ipykernel_19280\\2764605942.py:4: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "Positional arguments ['self', 'name', 'low', 'high', 'step', 'log'] in suggest_int() have been deprecated since v3.5.0. They will be replaced with the corresponding keyword arguments in v5.0.0, so please use the keyword specification instead. See https://github.com/optuna/optuna/releases/tag/v3.5.0 for details.\n",
      "  max_depth = trial.suggest_int('max_depth', 5, 30, 1)\n",
      "C:\\Users\\HYPERPC\\AppData\\Local\\Temp\\ipykernel_19280\\2764605942.py:5: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "Positional arguments ['self', 'name', 'low', 'high', 'step', 'log'] in suggest_int() have been deprecated since v3.5.0. They will be replaced with the corresponding keyword arguments in v5.0.0, so please use the keyword specification instead. See https://github.com/optuna/optuna/releases/tag/v3.5.0 for details.\n",
      "  min_samples_leaf = trial.suggest_int('min_samples_leaf', 2, 10, 1)\n",
      "[I 2025-10-27 12:55:57,628] Trial 19 finished with value: 0.9914057704112953 and parameters: {'n_estimators': 200, 'max_depth': 27, 'min_samples_leaf': 2}. Best is trial 8 with value: 0.9920245398773007.\n"
     ]
    }
   ],
   "source": [
    "study = optuna.create_study(study_name=\"RandomForestClassifier\", direction=\"maximize\")\n",
    "# ищем лучшую комбинацию гиперпараметров n_trials раз\n",
    "study.optimize(optuna_rf, n_trials=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "40c57773",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Наилучшие значения гиперпараметров: {'n_estimators': 280, 'max_depth': 28, 'min_samples_leaf': 2}\n",
      "f1_score на тестовом наборе: 0.80\n"
     ]
    }
   ],
   "source": [
    "# выводим результаты на обучающей выборке\n",
    "print(\"Наилучшие значения гиперпараметров: {}\".format(study.best_params))\n",
    "\n",
    "# рассчитаем точность для тестовой выборки\n",
    "model = ensemble.RandomForestClassifier(**study.best_params, random_state=random_state)\n",
    "model.fit(X_train, y_train)\n",
    "y_train_pred = model.predict(X_train)\n",
    "y_test_pred = model.predict(X_test)\n",
    "print(\"f1_score на тестовом наборе: {:.2f}\".format(metrics.f1_score(y_test, y_test_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b9e1356d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\HYPERPC\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "[I 2025-10-27 12:57:10,110] Trial 20 finished with value: 0.8739038403386755 and parameters: {'penalty': 'l2', 'solver': 'sag', 'C': 0.28834256235679145}. Best is trial 8 with value: 0.9920245398773007.\n",
      "c:\\Users\\HYPERPC\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "[I 2025-10-27 12:57:10,462] Trial 21 finished with value: 0.8909090909090909 and parameters: {'penalty': 'l2', 'solver': 'lbfgs', 'C': 0.9754450186923722}. Best is trial 8 with value: 0.9920245398773007.\n",
      "c:\\Users\\HYPERPC\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "[I 2025-10-27 12:57:13,186] Trial 22 finished with value: 0.8410041841004184 and parameters: {'penalty': 'l2', 'solver': 'sag', 'C': 0.045222482205623304}. Best is trial 8 with value: 0.9920245398773007.\n",
      "c:\\Users\\HYPERPC\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "[I 2025-10-27 12:57:13,526] Trial 23 finished with value: 0.8910470409711685 and parameters: {'penalty': 'l2', 'solver': 'lbfgs', 'C': 0.852985529292676}. Best is trial 8 with value: 0.9920245398773007.\n",
      "c:\\Users\\HYPERPC\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "[I 2025-10-27 12:57:16,224] Trial 24 finished with value: 0.883495145631068 and parameters: {'penalty': 'l2', 'solver': 'sag', 'C': 0.5877728321417188}. Best is trial 8 with value: 0.9920245398773007.\n",
      "c:\\Users\\HYPERPC\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "[I 2025-10-27 12:57:16,558] Trial 25 finished with value: 0.8855842185128984 and parameters: {'penalty': 'l2', 'solver': 'lbfgs', 'C': 0.5935488433346483}. Best is trial 8 with value: 0.9920245398773007.\n",
      "c:\\Users\\HYPERPC\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "[I 2025-10-27 12:57:19,328] Trial 26 finished with value: 0.8386132695756127 and parameters: {'penalty': 'l2', 'solver': 'sag', 'C': 0.03784493786820087}. Best is trial 8 with value: 0.9920245398773007.\n",
      "c:\\Users\\HYPERPC\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "[I 2025-10-27 12:57:19,679] Trial 27 finished with value: 0.877841770233404 and parameters: {'penalty': 'l2', 'solver': 'lbfgs', 'C': 0.3506717307425872}. Best is trial 8 with value: 0.9920245398773007.\n",
      "c:\\Users\\HYPERPC\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "[I 2025-10-27 12:57:20,004] Trial 28 finished with value: 0.8876098152075129 and parameters: {'penalty': 'l2', 'solver': 'lbfgs', 'C': 0.6793168511961979}. Best is trial 8 with value: 0.9920245398773007.\n",
      "c:\\Users\\HYPERPC\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "[I 2025-10-27 12:57:22,728] Trial 29 finished with value: 0.8764385221078135 and parameters: {'penalty': 'l2', 'solver': 'sag', 'C': 0.3230155906622242}. Best is trial 8 with value: 0.9920245398773007.\n",
      "c:\\Users\\HYPERPC\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "[I 2025-10-27 12:57:23,089] Trial 30 finished with value: 0.8880800727934486 and parameters: {'penalty': 'l2', 'solver': 'lbfgs', 'C': 0.7937467132914057}. Best is trial 8 with value: 0.9920245398773007.\n",
      "c:\\Users\\HYPERPC\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "[I 2025-10-27 12:57:25,798] Trial 31 finished with value: 0.8653208797830672 and parameters: {'penalty': 'l2', 'solver': 'sag', 'C': 0.2011555843564965}. Best is trial 8 with value: 0.9920245398773007.\n",
      "c:\\Users\\HYPERPC\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "[I 2025-10-27 12:57:26,153] Trial 32 finished with value: 0.883495145631068 and parameters: {'penalty': 'l2', 'solver': 'lbfgs', 'C': 0.5015859538611971}. Best is trial 8 with value: 0.9920245398773007.\n",
      "c:\\Users\\HYPERPC\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "[I 2025-10-27 12:57:28,870] Trial 33 finished with value: 0.8789808917197452 and parameters: {'penalty': 'l2', 'solver': 'sag', 'C': 0.43266707888335804}. Best is trial 8 with value: 0.9920245398773007.\n",
      "c:\\Users\\HYPERPC\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "[I 2025-10-27 12:57:29,203] Trial 34 finished with value: 0.8924665856622114 and parameters: {'penalty': 'l2', 'solver': 'lbfgs', 'C': 0.9934823669670992}. Best is trial 8 with value: 0.9920245398773007.\n",
      "c:\\Users\\HYPERPC\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "[I 2025-10-27 12:57:32,010] Trial 35 finished with value: 0.8841019417475728 and parameters: {'penalty': 'l2', 'solver': 'sag', 'C': 0.7416587817455647}. Best is trial 8 with value: 0.9920245398773007.\n",
      "c:\\Users\\HYPERPC\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "[I 2025-10-27 12:57:32,356] Trial 36 finished with value: 0.8620481927710844 and parameters: {'penalty': 'l2', 'solver': 'lbfgs', 'C': 0.15696621051814846}. Best is trial 8 with value: 0.9920245398773007.\n",
      "c:\\Users\\HYPERPC\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "[I 2025-10-27 12:57:35,023] Trial 37 finished with value: 0.8841019417475728 and parameters: {'penalty': 'l2', 'solver': 'sag', 'C': 0.9183892989879727}. Best is trial 8 with value: 0.9920245398773007.\n",
      "c:\\Users\\HYPERPC\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "[I 2025-10-27 12:57:35,368] Trial 38 finished with value: 0.8641975308641975 and parameters: {'penalty': 'l2', 'solver': 'lbfgs', 'C': 0.18507186189198377}. Best is trial 8 with value: 0.9920245398773007.\n",
      "c:\\Users\\HYPERPC\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "[I 2025-10-27 12:57:38,097] Trial 39 finished with value: 0.8841722255912674 and parameters: {'penalty': 'l2', 'solver': 'sag', 'C': 0.6254054324685827}. Best is trial 8 with value: 0.9920245398773007.\n",
      "c:\\Users\\HYPERPC\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "[I 2025-10-27 12:57:40,767] Trial 40 finished with value: 0.879247572815534 and parameters: {'penalty': 'l2', 'solver': 'sag', 'C': 0.4340474225576536}. Best is trial 8 with value: 0.9920245398773007.\n",
      "c:\\Users\\HYPERPC\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "[I 2025-10-27 12:57:41,107] Trial 41 finished with value: 0.8923263572945102 and parameters: {'penalty': 'l2', 'solver': 'lbfgs', 'C': 0.8483719866446922}. Best is trial 8 with value: 0.9920245398773007.\n",
      "c:\\Users\\HYPERPC\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "[I 2025-10-27 12:57:43,811] Trial 42 finished with value: 0.8800485879137565 and parameters: {'penalty': 'l2', 'solver': 'sag', 'C': 0.502863237280041}. Best is trial 8 with value: 0.9920245398773007.\n",
      "c:\\Users\\HYPERPC\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "[I 2025-10-27 12:57:44,135] Trial 43 finished with value: 0.8582582582582583 and parameters: {'penalty': 'l2', 'solver': 'lbfgs', 'C': 0.12277539160523848}. Best is trial 8 with value: 0.9920245398773007.\n",
      "c:\\Users\\HYPERPC\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "[I 2025-10-27 12:57:44,455] Trial 44 finished with value: 0.8890236506973923 and parameters: {'penalty': 'l2', 'solver': 'lbfgs', 'C': 0.7114321090704077}. Best is trial 8 with value: 0.9920245398773007.\n"
     ]
    }
   ],
   "source": [
    "study.optimize(optuna_lr, n_trials=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5a53154e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Наилучшие значения гиперпараметров: {'n_estimators': 280, 'max_depth': 28, 'min_samples_leaf': 2}\n",
      "f1_score на тестовом наборе: 0.80\n"
     ]
    }
   ],
   "source": [
    "# выводим результаты на обучающей выборке\n",
    "print(\"Наилучшие значения гиперпараметров: {}\".format(study.best_params))\n",
    "\n",
    "# рассчитаем точность для тестовой выборки\n",
    "model = ensemble.RandomForestClassifier(**study.best_params, random_state=random_state)\n",
    "model.fit(X_train, y_train)\n",
    "y_train_pred = model.predict(X_train)\n",
    "y_test_pred = model.predict(X_test)\n",
    "print(\"f1_score на тестовом наборе: {:.2f}\".format(metrics.f1_score(y_test, y_test_pred)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
